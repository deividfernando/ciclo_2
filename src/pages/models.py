import streamlit as st
import pandas as pd
from src.components.metrics import MetricCard
from src.components.charts import MLMetricsCharts
from src.config.constants import MODELS_INFO
from src.config.styles import COLOR_PALETTES

def show_models():
    """Renderiza a p√°gina de modelos e suas configura√ß√µes"""
    
    # Cabe√ßalho
    st.title("‚öôÔ∏è Modelos de Machine Learning")
    
    # Introdu√ß√£o
    st.markdown("""
    Esta se√ß√£o apresenta os detalhes dos modelos de ensemble utilizados no projeto.
    Cada modelo possui suas caracter√≠sticas espec√≠ficas e diferentes configura√ß√µes de hiperpar√¢metros.
    """)
    
    # Tabs para diferentes aspectos dos modelos
    tabs = st.tabs([
        "Vis√£o Geral",
        "Configura√ß√µes",
        "Hiperpar√¢metros",
        "Documenta√ß√£o"
    ])
    
    # Tab de Vis√£o Geral
    with tabs[0]:
        st.markdown("### üìã Modelos Implementados")
        
        models_overview = {
            "Random Forest": {
                "what_is": """
                    Funciona como uma "vota√ß√£o" entre v√°rios modelos, onde cada √°rvore analisa os dados 
                    de forma independente, e a decis√£o final √© baseada no consenso.
                """,
                "implementation": """
                    Em nossa implementa√ß√£o, configuramos com 300 √°rvores, profundidade m√°xima de 30 
                    e valida√ß√£o cruzada com 5 folds.
                """,
                "advantages": [
                    "Alta resist√™ncia ao overfitting",
                    "Excelente interpretabilidade",
                    "Robusto com diferentes tipos de dados",
                    "N√£o precisa de normaliza√ß√£o dos dados"
                ],
                "disadvantages": [
                    "Pode ser mais lento em datasets muito grandes",
                    "Maior uso de mem√≥ria RAM",
                    "Menos eficiente com rela√ß√µes muito complexas"
                ],
                "color": MODELS_INFO["Random Forest"]["color"]
            },
            "XGBoost": {
                "what_is": """
                    Trabalha com aprendizado sequencial e gradual, onde cada etapa corrige os erros 
                    da anterior, sendo muito eficiente computacionalmente.
                """,
                "implementation": """
                    Utilizamos learning rate de 0,1 e 300 estimadores, com early stopping para 
                    prevenir overfitting. Otimizado para alta precis√£o.
                """,
                "advantages": [
                    "Alta precis√£o na maioria dos problemas",
                    "Eficiente com grandes volumes de dados",
                    "Excelente em detectar padr√µes complexos",
                    "Implementa√ß√£o otimizada e r√°pida"
                ],
                "disadvantages": [
                    "Requer mais ajuste fino de par√¢metros",
                    "Maior complexidade de configura√ß√£o",
                    "Pode ser mais suscet√≠vel a overfitting"
                ],
                "color": MODELS_INFO["XGBoost"]["color"]
            },
            "LightGBM": {
                "what_is": """
                    Modelo com crescimento lateral das √°rvores, otimizado para performance e 
                    efici√™ncia computacional.
                """,
                "implementation": """
                    Configurado com 200 estimadores e 50 leaves por √°rvore, utilizando 5 folds 
                    na valida√ß√£o cruzada. Foco em velocidade e efici√™ncia.
                """,
                "advantages": [
                    "Excelente velocidade de treinamento",
                    "Uso eficiente de mem√≥ria",
                    "√ìtimo para datasets muito grandes",
                    "Bom equil√≠brio entre performance e velocidade"
                ],
                "disadvantages": [
                    "Menos eficiente com rela√ß√µes muito complexas",
                    "Maior uso de mem√≥ria RAM",
                    "Menos eficiente com rela√ß√µes muito complexas"
                ],
                "color": MODELS_INFO["LightGBM"]["color"]
            }
        }
        
        for name, info in models_overview.items():
            with st.expander(name, expanded=True):
                st.markdown(f"""
                <div style="
                    border-left: 4px solid {info['color']};
                    padding-left: 20px;
                ">
                    <p>{info['what_is']}</p>
                    <h4>Principais Caracter√≠sticas:</h4>
                    <ul>
                        {''.join(f'<li>{param}</li>' for param in info['advantages'])}
                    </ul>
                    <h4>Desvantagens:</h4>
                    <ul>
                        {''.join(f'<li>{param}</li>' for param in info['disadvantages'])}
                    </ul>
                </div>
                """, unsafe_allow_html=True)
        
        # Comparativo r√°pido
        st.markdown("### üîÑ Comparativo R√°pido")
        
        comparison_data = {
            "Random Forest": {
                "Velocidade": "‚≠ê‚≠ê‚≠ê",
                "Interpretabilidade": "‚≠ê‚≠ê‚≠ê‚≠ê",
                "Tunabilidade": "‚≠ê‚≠ê‚≠ê",
                "Performance": "‚≠ê‚≠ê‚≠ê‚≠ê"
            },
            "XGBoost": {
                "Velocidade": "‚≠ê‚≠ê‚≠ê‚≠ê",
                "Interpretabilidade": "‚≠ê‚≠ê‚≠ê",
                "Tunabilidade": "‚≠ê‚≠ê‚≠ê‚≠ê",
                "Performance": "‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê"
            },
            "LightGBM": {
                "Velocidade": "‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê",
                "Interpretabilidade": "‚≠ê‚≠ê‚≠ê",
                "Tunabilidade": "‚≠ê‚≠ê‚≠ê‚≠ê",
                "Performance": "‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê"
            }
        }
        
        comparison_df = pd.DataFrame(comparison_data).T
        st.dataframe(comparison_df)
    
    # Tab de Configura√ß√µes
    with tabs[1]:
        st.markdown("### ‚öôÔ∏è Configura√ß√µes dos Modelos")
        
        # Sele√ß√£o do modelo
        selected_model = st.selectbox(
            "Selecione o modelo para ver as configura√ß√µes",
            list(MODELS_INFO.keys())
        )
        
        if selected_model:
            # Par√¢metros padr√£o
            st.markdown("#### Par√¢metros Padr√£o")
            if selected_model == "Random Forest":
                st.code("""
                RandomForestClassifier(
                    n_estimators=100,
                    max_depth=None,
                    min_samples_split=2,
                    min_samples_leaf=1,
                    max_features='sqrt',
                    bootstrap=True,
                    random_state=42
                )
                """)
            elif selected_model == "XGBoost":
                st.code("""
                XGBClassifier(
                    n_estimators=100,
                    max_depth=6,
                    learning_rate=0.3,
                    subsample=1.0,
                    colsample_bytree=1.0,
                    random_state=42
                )
                """)
            elif selected_model == "LightGBM":
                st.code("""
                LGBMClassifier(
                    n_estimators=100,
                    num_leaves=31,
                    max_depth=-1,
                    learning_rate=0.1,
                    random_state=42
                )
                """)
            
            # Recomenda√ß√µes de uso
            st.markdown("#### üí° Recomenda√ß√µes de Uso")
            recommendations = {
                "Random Forest": [
                    "Ideal para datasets de tamanho m√©dio",
                    "Quando a interpretabilidade √© importante",
                    "Para casos com muitas features categ√≥ricas",
                    "Quando o overfitting √© uma preocupa√ß√£o"
                ],
                "XGBoost": [
                    "Para datasets grandes",
                    "Quando a performance √© cr√≠tica",
                    "Em competi√ß√µes de ML",
                    "Para casos com features num√©ricas"
                ],
                "LightGBM": [
                    "Para datasets muito grandes",
                    "Quando a velocidade √© crucial",
                    "Em ambientes com recursos limitados",
                    "Para treinamento distribu√≠do"
                ]
            }
            
            for rec in recommendations[selected_model]:
                st.markdown(f"- {rec}")
    
    # Tab de Hiperpar√¢metros
    with tabs[2]:
        st.markdown("### üéõÔ∏è Otimiza√ß√£o de Hiperpar√¢metros")
        
        # Sele√ß√£o do modelo
        model_for_tuning = st.selectbox(
            "Selecione o modelo para ver os hiperpar√¢metros",
            list(MODELS_INFO.keys()),
            key="tuning"
        )
        
        # Grid de busca
        if model_for_tuning:
            st.markdown("#### üîç Grid de Busca")
            
            if model_for_tuning == "Random Forest":
                param_grid = {
                    'n_estimators': [100, 200, 300],
                    'max_depth': [10, 20, 30, None],
                    'min_samples_split': [2, 5, 10],
                    'min_samples_leaf': [1, 2, 4]
                }
            elif model_for_tuning == "XGBoost":
                param_grid = {
                    'n_estimators': [100, 200, 300],
                    'max_depth': [3, 5, 7],
                    'learning_rate': [0.01, 0.1, 0.3],
                    'subsample': [0.8, 1.0]
                }
            elif model_for_tuning == "LightGBM":
                param_grid = {
                    'n_estimators': [100, 200, 300],
                    'num_leaves': [31, 50, 100],
                    'max_depth': [-1, 5, 10],
                    'learning_rate': [0.01, 0.1, 0.3]
                }
            
            st.json(param_grid)
            
            # Dicas de tunagem
            st.markdown("#### üí° Dicas de Tunagem")
            
            tuning_tips = {
                "Random Forest": [
                    "Comece com `n_estimators` mais alto",
                    "Ajuste `max_depth` para controlar overfitting",
                    "Balanceie `min_samples_split` e `min_samples_leaf`"
                ],
                "XGBoost": [
                    "Use `learning_rate` menor com mais `n_estimators`",
                    "Ajuste `max_depth` para controlar complexidade",
                    "Use `subsample` para reduzir overfitting"
                ],
                "LightGBM": [
                    "Prefira ajustar `num_leaves` a `max_depth`",
                    "Mantenha `learning_rate` pequeno inicialmente",
                    "Use `early_stopping` durante o treinamento"
                ]
            }
            
            for tip in tuning_tips[model_for_tuning]:
                st.markdown(f"- {tip}")
    
    # Tab de Documenta√ß√£o
    with tabs[3]:
        st.markdown("### üìö Documenta√ß√£o")
        
        # Links para documenta√ß√£o
        st.markdown("""
        #### üîó Links √öteis
        
        **Random Forest**:
        - [Scikit-learn Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)
        - [Random Forest User Guide](https://scikit-learn.org/stable/modules/ensemble.html#forest)
        
        **XGBoost**:
        - [XGBoost Documentation](https://xgboost.readthedocs.io/)
        - [XGBoost Parameters](https://xgboost.readthedocs.io/en/latest/parameter.html)
        
        **LightGBM**:
        - [LightGBM Documentation](https://lightgbm.readthedocs.io/)
        - [LightGBM Parameters](https://lightgbm.readthedocs.io/en/latest/Parameters.html)
        """)
        
        # Exemplo de c√≥digo
        st.markdown("#### üíª Exemplo de Implementa√ß√£o")
        
        st.code("""
        from sklearn.ensemble import RandomForestClassifier
        from xgboost import XGBClassifier
        from lightgbm import LGBMClassifier
        from sklearn.model_selection import RandomizedSearchCV

        # Definir modelo e grid de par√¢metros
        model = RandomForestClassifier(random_state=42)
        param_grid = {
            'n_estimators': [100, 200, 300],
            'max_depth': [10, 20, 30, None],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4]
        }

        # Realizar busca randomizada
        random_search = RandomizedSearchCV(
            model,
            param_distributions=param_grid,
            n_iter=10,
            cv=5,
            random_state=42,
            n_jobs=-1
        )

        # Treinar modelo
        random_search.fit(X_train, y_train)

        # Obter melhores par√¢metros
        best_params = random_search.best_params_
        """)
        
        # Notas importantes
        st.markdown("#### ‚ö†Ô∏è Notas Importantes")
        st.info("""
        - Sempre use valida√ß√£o cruzada para avaliar os modelos
        - Monitore o overfitting durante o treinamento
        - Considere o trade-off entre performance e tempo de treinamento
        - Mantenha um conjunto de teste separado para avalia√ß√£o final
        """)

    # Footer com informa√ß√µes adicionais
    st.markdown("""
    <div style='text-align: center; color: #666; padding: 20px;'>
        <small>
            Para mais informa√ß√µes sobre implementa√ß√£o e configura√ß√£o dos modelos,<br>
            consulte a documenta√ß√£o oficial de cada biblioteca.
        </small>
    </div>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    show_models()